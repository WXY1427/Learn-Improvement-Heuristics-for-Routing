{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboard_logger import Logger as TbLogger\n",
    "\n",
    "from critic_network import CriticNetwork\n",
    "from options import get_options\n",
    "from train import train_epoch, get_inner_model   ##########  no need for validate now\n",
    "from baselines import NoBaseline, ExponentialBaseline, CriticBaseline, RolloutBaseline, WarmupBaseline\n",
    "from attention_model import AttentionModel\n",
    "from utils import torch_load_cpu, load_model, maybe_cuda_model, load_problem\n",
    "from itertools import combinations, permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseline': 'critic',\n",
      " 'batch_size': 2,\n",
      " 'checkpoint_epochs': 5,\n",
      " 'embedding_dim': 128,\n",
      " 'epoch_size': 20,\n",
      " 'epoch_start': 0,\n",
      " 'eval_batch_size': 900,\n",
      " 'eval_only': False,\n",
      " 'graph_size': 50,\n",
      " 'hidden_dim': 128,\n",
      " 'lambda': 0.8,\n",
      " 'load_path': None,\n",
      " 'log_dir': 'logs',\n",
      " 'log_step': 50,\n",
      " 'lr_critic': 0.0001,\n",
      " 'lr_decay': 0.99,\n",
      " 'lr_model': 0.0001,\n",
      " 'max_grad_norm': 1.0,\n",
      " 'n_encode_layers': 3,\n",
      " 'n_epochs': 200,\n",
      " 'no_cuda': False,\n",
      " 'no_progress_bar': False,\n",
      " 'no_tensorboard': False,\n",
      " 'normalization': 'batch',\n",
      " 'output_dir': 'outputs',\n",
      " 'problem': 'cvrp',\n",
      " 'resume': 'outputs/cvrp_50/run/epoch-180.pt',\n",
      " 'run_name': 'run',\n",
      " 'save_dir': 'outputs/cvrp_50/run',\n",
      " 'seed': 1234,\n",
      " 'steps': 100,\n",
      " 'tanh_clipping': 10.0,\n",
      " 'use_cuda': True,\n",
      " 'val_dataset': None,\n",
      " 'val_size': 4500}\n",
      "  [*] Loading data from outputs/cvrp_50/run/epoch-180.pt\n",
      "4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming after 180\n",
      "Start train epoch 181, lr=0.0001 for run run\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 796.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 181, took 00:00:00 s\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaoxin/.local/lib/python3.5/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.64 GiB (GPU 0; 31.74 GiB total capacity; 14.81 GiB already allocated; 318.25 MiB free; 7.66 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c75df05a9e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mCL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         )   \n",
      "\u001b[0;32m~/myproject/jupyter-file/CVRP_06_T (50) (PPO)/train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, policy_old, optimizer, baseline, lr_scheduler, epoch, val_dataset, problem, tb_logger, dic, cl, opts)\u001b[0m\n\u001b[1;32m    296\u001b[0m         )\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mtotal_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     print('Improving: {} +- {}'.format(\n",
      "\u001b[0;32m~/myproject/jupyter-file/CVRP_06_T (50) (PPO)/train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(baseline, model, dataset, dic, cl, opts)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mche_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mche_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;31m#             pre_best = now_best.clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m#             now_best = torch.cat((pre_best[None,:], current_length[None,:]),0).min(0)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject/jupyter-file/CVRP_06_T (50) (PPO)/attention_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, rec, input_info, pos_enc, exc, che_mask, dic, cl, action, test)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# return (length_now-length_pre), None, length_now, dataset, rec_new from get_costs()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mnow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mche_mask\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_costs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexchange_for_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject/jupyter-file/CVRP_06_T (50) (PPO)/problems/problem_vrp.py\u001b[0m in \u001b[0;36mget_costs\u001b[0;34m(dataset, rec, dic, cl, exchange)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mche_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_tensor2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'demand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmov_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject/jupyter-file/CVRP_06_T (50) (PPO)/problems/problem_vrp.py\u001b[0m in \u001b[0;36mseq_tensor2\u001b[0;34m(input, rec)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mdep_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.64 GiB (GPU 0; 31.74 GiB total capacity; 14.81 GiB already allocated; 318.25 MiB free; 7.66 GiB cached)"
     ]
    }
   ],
   "source": [
    "pp.pprint(vars(opts))\n",
    "\n",
    "# Set the random seed\n",
    "torch.manual_seed(opts.seed)\n",
    "\n",
    "# Optionally configure tensorboard\n",
    "tb_logger = None\n",
    "if not opts.no_tensorboard:\n",
    "    tb_logger = TbLogger(os.path.join(opts.log_dir, \"{}_{}\".format(opts.problem, opts.graph_size), opts.run_name))\n",
    "\n",
    "\n",
    "# os.makedirs(opts.save_dir)\n",
    "# # Save arguments so exact configuration can always be found\n",
    "# with open(os.path.join(opts.save_dir, \"args.json\"), 'w') as f:\n",
    "#     json.dump(vars(opts), f, indent=True)\n",
    "\n",
    "# Figure out what's the problem\n",
    "problem = load_problem(opts.problem)\n",
    "\n",
    "# Load data from load_path\n",
    "load_data = {}\n",
    "assert opts.load_path is None or opts.resume is None, \"Only one of load path and resume can be given\"\n",
    "load_path = opts.load_path if opts.load_path is not None else opts.resume\n",
    "if load_path is not None:\n",
    "    print('  [*] Loading data from {}'.format(load_path))\n",
    "    load_data = load_data = torch_load_cpu(load_path)\n",
    "\n",
    "# Initialize model\n",
    "model_class = AttentionModel\n",
    "model = maybe_cuda_model(\n",
    "    model_class(\n",
    "        opts.embedding_dim,\n",
    "        opts.hidden_dim,\n",
    "        problem,\n",
    "        n_encode_layers=opts.n_encode_layers,\n",
    "        mask_inner=True,\n",
    "        mask_logits=True,\n",
    "        normalization=opts.normalization,\n",
    "        tanh_clipping=opts.tanh_clipping\n",
    "    ),\n",
    "    opts.use_cuda\n",
    ")\n",
    "\n",
    "# Overwrite model parameters by parameters to load\n",
    "model_ = get_inner_model(model)\n",
    "model_.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})\n",
    "\n",
    "\n",
    "# Initialize old policy\n",
    "model_old = AttentionModel\n",
    "policy_old = maybe_cuda_model(\n",
    "    model_old(\n",
    "        opts.embedding_dim,\n",
    "        opts.hidden_dim,\n",
    "        problem,\n",
    "        n_encode_layers=opts.n_encode_layers,\n",
    "        mask_inner=True,\n",
    "        mask_logits=True,\n",
    "        normalization=opts.normalization,\n",
    "        tanh_clipping=opts.tanh_clipping\n",
    "    ),\n",
    "    opts.use_cuda\n",
    ")\n",
    "\n",
    "policy_old.load_state_dict(model.state_dict())\n",
    "\n",
    "# Initialize baseline\n",
    "baseline = CriticBaseline(\n",
    "            maybe_cuda_model(\n",
    "                CriticNetwork(\n",
    "                    7,\n",
    "                    opts.embedding_dim,\n",
    "                    opts.hidden_dim,\n",
    "                    opts.n_encode_layers,\n",
    "                    opts.normalization\n",
    "                ),\n",
    "                opts.use_cuda\n",
    "            )\n",
    "        )\n",
    "# Load baseline from data, make sure script is called with same type of baseline\n",
    "if 'baseline' in load_data:\n",
    "    baseline.load_state_dict(load_data['baseline'])\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(\n",
    "    [{'params': model.parameters(), 'lr': opts.lr_model}]\n",
    "    + (\n",
    "        [{'params': baseline.get_learnable_parameters(), 'lr': opts.lr_critic}]\n",
    "        if len(baseline.get_learnable_parameters()) > 0\n",
    "        else []\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load optimizer state\n",
    "if 'optimizer' in load_data:\n",
    "    optimizer.load_state_dict(load_data['optimizer'])\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            # if isinstance(v, torch.Tensor):\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "# Initialize learning rate scheduler, decay by lr_decay once per epoch!\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: opts.lr_decay ** epoch)\n",
    "\n",
    "# Start the actual training loop\n",
    "val_dataset = problem.make_dataset(size=opts.graph_size, num_samples=opts.val_size, filename=opts.val_dataset)\n",
    "# torch.save(val_dataset, 'myval_CVRP_50.pt')\n",
    "\n",
    "# val_dataset = torch.load('myval_CVRP_50.pt')[0:1000]\n",
    "\n",
    "# prepare for 2opt mask\n",
    "o=torch.tensor([i for i in range(100)])\n",
    "oo = torch.tensor(o)\n",
    "CL=list(combinations(o, 2))\n",
    "lis=[]\n",
    "for i in CL:\n",
    "    ooo = oo.clone()\n",
    "    ooo[i[0]:i[1]+1]=torch.flip(oo[i[0]:i[1]+1],[0]).clone()\n",
    "    lis.append(ooo)\n",
    "dic = torch.stack(lis,0)\n",
    "dic=dic.cuda()\n",
    "CL = torch.tensor(CL).cuda()\n",
    "\n",
    "if opts.resume:\n",
    "    epoch_resume = int(os.path.splitext(os.path.split(opts.resume)[-1])[0].split(\"-\")[1])\n",
    "\n",
    "    torch.set_rng_state(load_data['rng_state'])\n",
    "    if opts.use_cuda:\n",
    "        torch.cuda.set_rng_state_all(load_data['cuda_rng_state'])\n",
    "    # Set the random states\n",
    "    # Dumping of state was done before epoch callback, so do that now (model is loaded)\n",
    "    baseline.epoch_callback(model, epoch_resume)\n",
    "    print(\"Resuming after {}\".format(epoch_resume))\n",
    "    opts.epoch_start = epoch_resume + 1\n",
    "\n",
    "if opts.eval_only:\n",
    "    validate(model, val_dataset, opts)\n",
    "else:\n",
    "    for epoch in range(opts.epoch_start, opts.epoch_start + opts.n_epochs):\n",
    "        train_epoch(\n",
    "            model,\n",
    "            policy_old,\n",
    "            optimizer,\n",
    "            baseline,\n",
    "            lr_scheduler,\n",
    "            epoch,\n",
    "            val_dataset,\n",
    "            problem,\n",
    "            tb_logger,\n",
    "            dic,\n",
    "            CL,\n",
    "            opts\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
